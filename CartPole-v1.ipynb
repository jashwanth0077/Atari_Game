{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e61a38-9a4a-4caf-9661-e0501b498a04",
   "metadata": {},
   "source": [
    "# Developing a Deep Q-Learning Agent for Atari Games\n",
    "\n",
    "**Jashwanth Kakara**  \n",
    "**22B1033**\n",
    "\n",
    "The main packages we require are **Numpy, Keras, Tensorflow, gym**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b94f8-2cda-40ee-8a0d-c5db430af6a9",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import necessary libraries (`gym` for the environment, `numpy` for numerical operations, `deque` for replay memory, `Sequential`, `Dense`, and `Adam` from `keras` for neural network modeling and optimization, `random` for random actions, and `matplotlib.pyplot` for plotting).\n",
    "\n",
    "### Constants\n",
    "\n",
    "Define constants such as `EPISODES` (number of training episodes), `MAX_STEPS` (maximum steps per episode), `BATCH_SIZE` (size of minibatch for replay), and `UPDATE_FREQ` (frequency of updating the target network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60da01d1-d1e5-4961-80ac-3c99450c812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPISODES = 500\n",
    "MAX_STEPS = 500\n",
    "BATCH_SIZE = 32\n",
    "UPDATE_FREQ = 5  # Update target network every UPDATE_FREQ steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df3fc7-b056-4ea2-a682-da8afdb782aa",
   "metadata": {},
   "source": [
    "### DQNAgent Class\n",
    "\n",
    "Represents the Deep Q-Network (DQN) agent.\n",
    "\n",
    "**`__init__` method** initializes the agent with state and action sizes, sets up memory for experience replay (`deque`), defines hyperparameters (gamma, epsilon, etc.), and builds the neural network model (`self.model`) using `_build_model()` method.\n",
    "\n",
    "**_build_model method** creates a simple neural network with 2 hidden layers (24 units each) and an output layer (action_size units) using `Sequential` from Keras. It uses ReLU activation for hidden layers and linear activation for the output layer. The model is compiled with Mean Squared Error loss and Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21bd316-a9f1-425b-b5cf-5ddb52337463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            history = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            loss = history.history['loss'][0]\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            return loss\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def evaluate(self, env, num_episodes):\n",
    "        total_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _, a = env.step(action)\n",
    "                total_reward += reward\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                state = next_state\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Evaluation Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        return total_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e38e2d-4578-4296-817e-13c299c6c643",
   "metadata": {},
   "source": [
    "### Memory Management\n",
    "\n",
    "**memorize method** stores the experience tuple `(state, action, reward, next_state, done)` in the replay memory (`self.memory`).\n",
    "\n",
    "### Action Selection (act method)\n",
    "\n",
    "**act method** selects an action based on the current state (state):\n",
    "- With probability epsilon, it chooses a random action (exploration).\n",
    "- Otherwise, it selects the action with the highest predicted value (exploitation) using the current neural network model (`self.model.predict`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a0472a-9ccc-4d3e-aeea-a3b11f807034",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a750911-81a4-41a0-adb6-babfbc69fb25",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Q-Learning is an off-policy algorithm where the goal is to learn the optimal action-value function \\( Q^*(s, a) \\) which satisfies the Bellman equation:\n",
    "\n",
    "$$ Q^*(s, a) = \\mathbb{E} \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a \\right] $$\n",
    "\n",
    "Here,\n",
    "- \\( r \\) is the reward,\n",
    "- \\( \\gamma \\) is the discount factor, and\n",
    "- \\( s' \\) is the next state.\n",
    "\n",
    "### Replay Buffer\n",
    "\n",
    "The Replay Buffer stores past experiences \\( (s, a, r, s', \\text{done}) \\) to break the temporal correlations and enable efficient reuse of experiences. A mini-batch is sampled from this buffer to update the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a89cac-b680-400a-a9d5-5c9a81a246e7",
   "metadata": {},
   "source": [
    "### CartPole-v1 Environment\n",
    "\n",
    "- **State Space**: The state consists of four values: cart position, cart velocity, pole angle, and pole velocity at the tip.\n",
    "- **Action Space**: Two discrete actions: move cart left or right.\n",
    "- **Termination Condition (done)**: The episode ends when:\n",
    "  - The pole falls past a certain angle.\n",
    "  - The cart moves out of bounds.\n",
    "  - The maximum number of steps per episode (typically 500) is reached.\n",
    "- **Reward**: The agent receives a reward of +1 for every step the pole remains upright and -10 if the pole falls (when `done` is True).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8cc374-1db7-430b-b769-1448b2691ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            history = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            loss = history.history['loss'][0]\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            return loss\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def evaluate(self, env, num_episodes):\n",
    "        total_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _, a = env.step(action)\n",
    "                total_reward += reward\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                state = next_state\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Evaluation Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedd46e-ba88-4902-8dbb-e1926607c7a6",
   "metadata": {},
   "source": [
    "### Main Execution (__main__ block)\n",
    "\n",
    "- Creates the **CartPole-v1** environment (`env`), initializes the agent (`agent`), and sets up a list (`episode_rewards`) to store average rewards for plotting.\n",
    "- Trains the agent over `EPISODES` episodes:\n",
    "  - Resets the environment (`env.reset()`) and initializes the state (`state`).\n",
    "  - Runs the agent's policy (`agent.act`) for up to `MAX_STEPS` steps per episode, collecting experiences (`agent.memorize`).\n",
    "  - Performs replay (`agent.replay`) to update the neural network based on experiences.\n",
    "  - Prints training progress (episode number, score, epsilon, loss) and evaluates (`agent.evaluate`) the agent's performance every 10 episodes, storing average rewards in `episode_rewards`.\n",
    "- Closes the environment (`env.close()`) after training completes.\n",
    "- Plots the average reward per episode (`episode_rewards`) using `matplotlib`."
   ]
